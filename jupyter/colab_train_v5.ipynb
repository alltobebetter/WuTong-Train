{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# æ¢§æ¡æ¯ AI å®‰å…¨å‘Šè­¦æ™ºèƒ½ç ”åˆ¤ç³»ç»Ÿ - V5 æœ€ç»ˆç‰ˆï¼ˆå†²å‡» 99.8%+ï¼‰\n",
        "\n",
        "## ğŸ¯ V5 æ ¸å¿ƒä¼˜åŒ–\n",
        "\n",
        "1. â­â­â­â­â­ **å¤–éƒ¨æ•°æ®é›†æ•´åˆ**ï¼šCSIC 2010 (36k+)ï¼Œé¢„æœŸ +0.5-1.0%\n",
        "2. â­â­â­â­â­ **SMOTE è¿‡é‡‡æ ·**ï¼šè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼Œé¢„æœŸ +0.8-1.2%\n",
        "3. â­â­â­â­ **XGBoost æ·±åº¦ä¼˜åŒ–**ï¼š500 æ ‘ + æ·±åº¦ 12ï¼Œé¢„æœŸ +0.3-0.5%\n",
        "4. ğŸ› **ä¿®å¤è­¦å‘Š**ï¼šç§»é™¤ scale_pos_weight å‚æ•°\n",
        "5. âš¡ **æ™ºèƒ½èšåˆå™¨**ï¼šå¤§æ•°æ®é›†ï¼ˆ>50kï¼‰è‡ªåŠ¨è·³è¿‡èšåˆä»¥èŠ‚çœæ—¶é—´\n",
        "6. âœ¨ **10 æŠ˜äº¤å‰éªŒè¯**ï¼šæ›´ç¨³å®šçš„æ€§èƒ½è¯„ä¼°\n",
        "\n",
        "## ğŸ“‹ è®­ç»ƒæµç¨‹\n",
        "\n",
        "1. âœ… å…‹éš† GitHub ä»“åº“\n",
        "2. âœ… å®‰è£…ä¾èµ–ï¼ˆå« imbalanced-learnï¼‰\n",
        "3. âœ… æ£€æŸ¥ GPU\n",
        "4. âœ… æ£€æŸ¥æ•°æ®\n",
        "5. âœ… æ•°æ®é¢„å¤„ç†\n",
        "6. âœ… æ•°æ®å¢å¼º\n",
        "7. âœ… **ä¸‹è½½å¹¶æ•´åˆ CSIC 2010 æ•°æ®é›†ï¼ˆV5 æ ¸å¿ƒï¼‰**\n",
        "8. âœ… **è®­ç»ƒ V5 æ¨¡å‹ï¼ˆSMOTE + å¤–éƒ¨æ•°æ®é›† + ä¼˜åŒ–è¶…å‚æ•°ï¼‰**\n",
        "9. âœ… æŸ¥çœ‹ç»“æœ\n",
        "10. âœ… ä¸‹è½½æ¨¡å‹\n",
        "\n",
        "## ğŸ¯ ç›®æ ‡\n",
        "\n",
        "- **å‡†ç¡®ç‡ç›®æ ‡**: 99.8%+\n",
        "- **V3 åŸºçº¿**: 98.36%\n",
        "- **V4 åŸºçº¿**: 99.2-99.5%\n",
        "- **é¢„æœŸæå‡**: +1.4-1.6%\n",
        "- **è®­ç»ƒæ—¶é—´**: çº¦ 40-50 åˆ†é’Ÿ\n",
        "- **æ¨¡å‹ç‰ˆæœ¬**: v5.0.0\n",
        "\n",
        "## âš™ï¸ è¿è¡Œå‰å‡†å¤‡\n",
        "\n",
        "1. ç¡®ä¿è¿è¡Œæ—¶ç±»å‹è®¾ç½®ä¸º **GPU**ï¼ˆRuntime > Change runtime type > GPUï¼‰\n",
        "2. æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªå•å…ƒæ ¼\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone"
      },
      "source": [
        "# 1. å…‹éš† GitHub ä»“åº“\n",
        "!git clone https://github.com/alltobebetter/WuTong.git\n",
        "%cd WuTong\n",
        "\n",
        "# æŸ¥çœ‹é¡¹ç›®ç»“æ„\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install"
      },
      "source": [
        "# 2. å®‰è£…ä¾èµ–ï¼ˆå« imbalanced-learnï¼‰\n",
        "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...\")\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"\\nâœ… ä¾èµ–å®‰è£…å®Œæˆï¼\")\n",
        "\n",
        "# éªŒè¯å…³é”®åŒ…\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print(f\"XGBoost ç‰ˆæœ¬: {xgb.__version__}\")\n",
        "print(f\"CatBoost ç‰ˆæœ¬: {cb.__version__}\")\n",
        "print(f\"LightGBM ç‰ˆæœ¬: {lgb.__version__}\")\n",
        "print(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")\n",
        "print(f\"âœ… SMOTE å·²å®‰è£…\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpu"
      },
      "source": [
        "# 3. æ£€æŸ¥ GPU å¯ç”¨æ€§\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰\")\n",
        "    print(\"   å»ºè®®: Runtime > Change runtime type > GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data"
      },
      "source": [
        "# 4. æ£€æŸ¥æ•°æ®é›†\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_files = list(Path('data').rglob('*.xlsx'))\n",
        "print(f\"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶:\")\n",
        "for f in data_files:\n",
        "    print(f\"  - {f}\")\n",
        "\n",
        "if data_files:\n",
        "    df = pd.read_excel(data_files[0])\n",
        "    print(f\"\\næ•°æ®é›†å¤§å°: {len(df)} æ¡\")\n",
        "    print(f\"\\næ”»å‡»ç±»å‹åˆ†å¸ƒ:\")\n",
        "    print(df.iloc[:, -1].value_counts())\n",
        "else:\n",
        "    print(\"\\nâš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preprocess"
      },
      "source": [
        "# 5. æ•°æ®é¢„å¤„ç†\n",
        "print(\"ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
        "\n",
        "import glob\n",
        "excel_files = glob.glob('data/**/*.xlsx', recursive=True)\n",
        "\n",
        "if excel_files:\n",
        "    data_file = excel_files[0]\n",
        "    print(f\"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}\")\n",
        "    !python scripts/ingest.py \"{data_file}\"\n",
        "    print(\"\\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
        "\n",
        "    parquet_files = glob.glob('data/staging/*.parquet')\n",
        "    print(f\"\\nç”Ÿæˆçš„ parquet æ–‡ä»¶: {parquet_files}\")\n",
        "else:\n",
        "    print(\"âŒ æœªæ‰¾åˆ° Excel æ•°æ®æ–‡ä»¶ï¼\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "augment"
      },
      "source": [
        "# 6. æ•°æ®å¢å¼º\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ æ•°æ®å¢å¼ºå¼€å§‹\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nç­–ç•¥:\")\n",
        "print(\"  - SQL æ³¨å…¥: å…³é”®è¯æ›¿æ¢ã€æ³¨é‡Šå˜æ¢ã€ç©ºæ ¼ç¼–ç \")\n",
        "print(\"  - XSS æ”»å‡»: æ ‡ç­¾å¤§å°å†™ã€äº‹ä»¶å¤„ç†å™¨ã€ç¼–ç å˜æ¢\")\n",
        "print(\"  - å‘½ä»¤æ³¨å…¥: åˆ†éš”ç¬¦å˜æ¢ã€å‘½ä»¤ç»„åˆ\")\n",
        "print(\"  - URL è·¯å¾„: å¤§å°å†™ã€ç¼–ç ã€è·¯å¾„åˆ†éš”ç¬¦\")\n",
        "print(\"\\nç›®æ ‡: 11,000 â†’ 30,000+ æ¡ (2.7x)\\n\")\n",
        "\n",
        "!python scripts/augment_data.py --target-size 30000 --ratio 2.5\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… æ•°æ®å¢å¼ºå®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# æŸ¥çœ‹å¢å¼ºåçš„æ•°æ®\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "augmented_files = glob.glob('data/staging/*augmented*.parquet')\n",
        "if augmented_files:\n",
        "    df_aug = pd.read_parquet(augmented_files[0])\n",
        "    print(f\"\\nğŸ“Š å¢å¼ºåç»Ÿè®¡:\")\n",
        "    print(f\"  æ€»æ•°æ®é‡: {len(df_aug)} æ¡\")\n",
        "    print(f\"  å¢å¼ºå€æ•°: {len(df_aug) / 11000:.2f}x\")\n",
        "    print(f\"\\nç±»åˆ«åˆ†å¸ƒ:\")\n",
        "    print(df_aug['attack_type'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csic2010"
      },
      "source": [
        "# 7. ä¸‹è½½å¹¶æ•´åˆ CSIC 2010 æ•°æ®é›†ï¼ˆV5 æ ¸å¿ƒï¼‰\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ ä¸‹è½½å¹¶æ•´åˆ CSIC 2010 æ•°æ®é›†\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\næ•°æ®é›†ä¿¡æ¯:\")\n",
        "print(\"  - æ¥æº: Spanish Research National Council (CSIC)\")\n",
        "print(\"  - è§„æ¨¡: 36,000+ æ­£å¸¸æµé‡ + 25,000+ æ”»å‡»æµé‡\")\n",
        "print(\"  - ç±»å‹: SQL æ³¨å…¥ã€XSSã€ç¼“å†²åŒºæº¢å‡ºã€ç›®å½•éå†ç­‰\")\n",
        "print(\"  - é¢„æœŸæå‡: +0.5-1.0%\\n\")\n",
        "\n",
        "!python scripts/integrate_csic2010.py\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… CSIC 2010 æ•´åˆå®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# æŸ¥çœ‹å¤–éƒ¨æ•°æ®é›†\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "external_files = glob.glob('data/external/*.parquet')\n",
        "if external_files:\n",
        "    df_ext = pd.read_parquet(external_files[0])\n",
        "    print(f\"\\nğŸ“Š å¤–éƒ¨æ•°æ®é›†ç»Ÿè®¡:\")\n",
        "    print(f\"  æ€»æ•°æ®é‡: {len(df_ext)} æ¡\")\n",
        "    print(f\"\\nç±»åˆ«åˆ†å¸ƒ:\")\n",
        "    print(df_ext['attack_type'].value_counts())\n",
        "    \n",
        "    # è®¡ç®—æ€»æ•°æ®é‡\n",
        "    augmented_files = glob.glob('data/staging/*augmented*.parquet')\n",
        "    if augmented_files:\n",
        "        df_aug = pd.read_parquet(augmented_files[0])\n",
        "        total = len(df_aug) + len(df_ext)\n",
        "        print(f\"\\nğŸ“ˆ æ€»æ•°æ®é‡: {total} æ¡ ({total / 11000:.1f}x)\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ æœªæ‰¾åˆ°å¤–éƒ¨æ•°æ®é›†\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train"
      },
      "source": [
        "# 8. è®­ç»ƒ V5 æ¨¡å‹ï¼ˆå¤–éƒ¨æ•°æ®é›† + SMOTE + ä¼˜åŒ–è¶…å‚æ•°ï¼‰\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ V5 æ¨¡å‹è®­ç»ƒå¼€å§‹ï¼ˆå¤–éƒ¨æ•°æ®é›† + SOTA ä¼˜åŒ–ï¼‰\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\né…ç½®:\")\n",
        "print(\"  - æ¨¡å‹: XGBoost + CatBoost + LightGBM\")\n",
        "print(\"  - é›†æˆæ–¹å¼: Stacking (ä¼˜åŒ– meta-learner)\")\n",
        "print(\"  - äº¤å‰éªŒè¯: 10 æŠ˜\")\n",
        "print(\"  - â­ å¤–éƒ¨æ•°æ®é›†: CSIC 2010 (36k+)\")\n",
        "print(\"  - â­ SMOTE è¿‡é‡‡æ ·: æ˜¯ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼‰\")\n",
        "print(\"  - â­ XGBoost ä¼˜åŒ–: 500 æ ‘ + æ·±åº¦ 12ï¼ˆä¿®å¤è­¦å‘Šï¼‰\")\n",
        "print(\"  - é¢„è®¡æ—¶é—´: 40-50 åˆ†é’Ÿ\\n\")\n",
        "\n",
        "import subprocess\n",
        "cmd = [\n",
        "    \"python\", \"-u\", \"scripts/train_v5.py\",\n",
        "    \"--version\", \"v5.0.0\",\n",
        "    \"--cv-splits\", \"10\"\n",
        "]\n",
        "proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
        "for line in proc.stdout:\n",
        "    print(line, end='')\n",
        "ret = proc.wait()\n",
        "if ret != 0:\n",
        "    raise RuntimeError(f\"è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {ret}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… V5 è®­ç»ƒå®Œæˆï¼\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "results"
      },
      "source": [
        "# 9. æŸ¥çœ‹ V5 è®­ç»ƒç»“æœ\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "manifest_path = Path('models/v5.0.0/manifest.json')\n",
        "\n",
        "if manifest_path.exists():\n",
        "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
        "        manifest = json.load(f)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ“Š V5 è®­ç»ƒç»“æœï¼ˆå¤–éƒ¨æ•°æ®é›† + SOTA ä¼˜åŒ–ï¼‰\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nç‰ˆæœ¬: {manifest['version']}\")\n",
        "    print(f\"è®­ç»ƒæ—¶é—´: {manifest['trained_at']}\")\n",
        "    print(f\"æ•°æ®é‡: {manifest['data_rows']} æ¡\")\n",
        "    print(f\"å¤–éƒ¨æ•°æ®é›†: {'æ˜¯' if manifest.get('external_data') else 'å¦'}\")\n",
        "    if manifest.get('external_data'):\n",
        "        print(f\"å¤–éƒ¨æ•°æ®é‡: {manifest.get('external_data_rows', 0)} æ¡\")\n",
        "    print(f\"ç‰¹å¾æ•°: {len(manifest['feature_list'])} ä¸ª\")\n",
        "    print(f\"ç±»åˆ«æ•°: {len(manifest['classes'])} ç±»\")\n",
        "\n",
        "    config = manifest.get('training_config', {})\n",
        "    print(f\"\\nè®­ç»ƒé…ç½®:\")\n",
        "    print(f\"  - äº¤å‰éªŒè¯: {config.get('n_cv_splits', 'N/A')} æŠ˜\")\n",
        "    print(f\"  - é›†æˆæ–¹å¼: {config.get('use_stacking', False) and 'Stacking' or 'Voting'}\")\n",
        "    print(f\"  - SMOTE è¿‡é‡‡æ ·: {config.get('use_smote', False) and 'æ˜¯' or 'å¦'}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ¯ æ¨¡å‹æ€§èƒ½\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    metrics = manifest['metrics']\n",
        "\n",
        "    # å•æ¨¡å‹æ€§èƒ½\n",
        "    for model_name in ['xgboost', 'catboost', 'lightgbm']:\n",
        "        if model_name in metrics:\n",
        "            m = metrics[model_name]\n",
        "            print(f\"\\n{model_name.upper()}:\")\n",
        "            print(f\"  æµ‹è¯•å‡†ç¡®ç‡: {m['test_accuracy']:.4f} ({m['test_accuracy']*100:.2f}%)\")\n",
        "            print(f\"  æµ‹è¯• F1: {m['test_f1']:.4f}\")\n",
        "            if m.get('cv_accuracy'):\n",
        "                print(f\"  CV å‡†ç¡®ç‡: {m['cv_accuracy']:.4f} (Â±{m.get('cv_std', 0):.4f})\")\n",
        "\n",
        "    # é›†æˆæ¨¡å‹æ€§èƒ½\n",
        "    if 'ensemble' in metrics:\n",
        "        e = metrics['ensemble']\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ† V5 é›†æˆæ¨¡å‹ï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  å‡†ç¡®ç‡: {e['test_accuracy']:.4f} ({e['test_accuracy']*100:.2f}%)\")\n",
        "        print(f\"  F1 åˆ†æ•°: {e['test_f1']:.4f}\")\n",
        "        print(f\"  é›†æˆæ–¹å¼: {e.get('ensemble_type', 'N/A')}\")\n",
        "\n",
        "        # ä¸ V3/V4 å¯¹æ¯”\n",
        "        v3_acc = 0.9836  # V3 çš„å‡†ç¡®ç‡\n",
        "        v4_acc = 0.9920  # V4 é¢„æœŸå‡†ç¡®ç‡ï¼ˆå‡è®¾ï¼‰\n",
        "        improvement_v3 = (e['test_accuracy'] - v3_acc) * 100\n",
        "        improvement_v4 = (e['test_accuracy'] - v4_acc) * 100\n",
        "        print(f\"\\n  ğŸ“ˆ ç›¸æ¯” V3 æå‡: {improvement_v3:+.2f}%\")\n",
        "        print(f\"  ğŸ“ˆ ç›¸æ¯” V4 æå‡: {improvement_v4:+.2f}%\")\n",
        "\n",
        "        if e['test_accuracy'] >= 0.998:\n",
        "            print(f\"\\n  ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼è¾¾åˆ° 99.8% ç›®æ ‡ï¼\")\n",
        "        elif e['test_accuracy'] >= 0.995:\n",
        "            print(f\"\\n  ğŸ‰ğŸ‰ æ­å–œï¼è¾¾åˆ° 99.5% ç›®æ ‡ï¼\")\n",
        "        elif e['test_accuracy'] >= 0.99:\n",
        "            print(f\"\\n  ğŸ‰ æ­å–œï¼è¾¾åˆ° 99% ç›®æ ‡ï¼\")\n",
        "        elif e['test_accuracy'] >= 0.985:\n",
        "            print(f\"\\n  âœ¨ éå¸¸æ¥è¿‘ 99%ï¼Œè¡¨ç°ä¼˜ç§€ï¼\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
        "    report_path = Path('models/v5.0.0/classification_report.txt')\n",
        "    if report_path.exists():\n",
        "        print(\"\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
        "        print(\"=\"*70)\n",
        "        with open(report_path, 'r', encoding='utf-8') as f:\n",
        "            print(f.read())\n",
        "else:\n",
        "    print(\"âŒ æœªæ‰¾åˆ° V5 è®­ç»ƒç»“æœæ–‡ä»¶ï¼\")\n",
        "    print(\"è¯·å…ˆè¿è¡Œè®­ç»ƒå•å…ƒæ ¼\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download"
      },
      "source": [
        "# 10. æ‰“åŒ…å¹¶ä¸‹è½½æ¨¡å‹\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "model_dir = Path('models/v5.0.0')\n",
        "if model_dir.exists():\n",
        "    print(\"ğŸ“¦ æ‰“åŒ…æ¨¡å‹æ–‡ä»¶...\")\n",
        "    shutil.make_archive('models_v5.0.0', 'zip', 'models', 'v5.0.0')\n",
        "    print(\"âœ… æ‰“åŒ…å®Œæˆ: models_v5.0.0.zip\")\n",
        "    \n",
        "    # åœ¨ Colab ä¸­ä¸‹è½½\n",
        "    from google.colab import files\n",
        "    files.download('models_v5.0.0.zip')\n",
        "    print(\"\\nâœ… ä¸‹è½½å·²å¼€å§‹ï¼\")\n",
        "else:\n",
        "    print(\"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}